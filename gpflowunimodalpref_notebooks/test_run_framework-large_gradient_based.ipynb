{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Thermal Preference Elicitation Framework\n",
    "\n",
    "Currently, this framework has been validated for only 1D (operating temp) feature./"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1\n",
    "\n",
    "Load required modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nimishawalgaonkar/anaconda2/lib/python2.7/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "import scipy\n",
    "sns.set_context(\"talk\", font_scale = 1.4)\n",
    "from GPFlowUnimodalPref.GPUnimodalElicit import elicit\n",
    "from GPFlowUnimodalPref.SynOccupant import datagen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2\n",
    "\n",
    "1. Occupant walks inside the room and is exposed to state r.\n",
    "2. Randomly change the state of the room to new state s.\n",
    "3. Ask the occupant which state they prefer. \n",
    "4. If they say state current state s, then record y as 1. If they say previous state r, then record y as 0.\n",
    "This first duel is our initial duel.\n",
    "5. Put the duel value and response in the '../data/duels/duels.csv' file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load the duels file\n",
    "data_file = '../GPFlowUnimodalPref/data/initial_duels/gradtrain1D.npz' # <--- you need to keep on updating this file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = np.load(data_file)\n",
    "X = data['X']\n",
    "Y = data['Y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "iter_num = X.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Tprev = np.array(data.Tprev) # previous state (operating temp.)\n",
    "#Tcurrent = np.array(data.Tcurrent) # current state <---- elicitation framework will give you this values\n",
    "#X = np.vstack([[Tprev, Tcurrent]]).T\n",
    "#X = X.astype(float) # features need to be float\n",
    "X_prime = np.linspace(20, 30, 10)[:,None]\n",
    "#Y = np.array(data.y)[:,None] # response of the occupant <---- you need to ask occupant about this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40,)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3\n",
    "\n",
    "Now, we want to find the next elicited state. So, we need to use **elicit** module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "config_file = '../GPFlowUnimodalPref/config_files/thermal_config.json' # configuration for grid (how fine you want to be)\n",
    "trial_num = 10 # this is just to save all the plots in '../data/results/T1' if trial_num = 1 ; T2 if trial_num = 2\n",
    "model_num = 2\n",
    "mcmc = True\n",
    "reachable = True\n",
    "savefig = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Model is 2\n",
      "----------------------------------------\n",
      "burn-in sampling started\n",
      "Iteration:  100 \t Acc Rate:  0.0 %\n",
      "Iteration:  200 \t Acc Rate:  0.0 %\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-894e4c2e2d85>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mAq\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0melicit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIntegratedAquisition\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_prime\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_num\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmcmc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreachable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/nimishawalgaonkar/Desktop/preferential_bayesian_optimization/repos/repos2018/FALL18_PREF_LEARNING/GPFlowUnimodalPref/GPUnimodalElicit/elicit.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, X, Y, X_prime, config_file, model_num, mcmc, reachable)\u001b[0m\n\u001b[1;32m     43\u001b[0m                       config_file, self.model_num)\n\u001b[1;32m     44\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmcmc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msamples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTRAIN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmcmc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTRAIN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaxpost\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/nimishawalgaonkar/Desktop/preferential_bayesian_optimization/repos/repos2018/FALL18_PREF_LEARNING/GPFlowUnimodalPref/GPUnimodalPref/train.pyc\u001b[0m in \u001b[0;36mmcmc\u001b[0;34m(self, config_file)\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_m\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaxiter\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mMAP_optimize_maxiter\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# start near MAP\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m         samples = self._m.sample(num_samples, verbose= verbose,\n\u001b[0;32m---> 82\u001b[0;31m                            epsilon= epsilon, thin = thin, burn = burn, Lmax=Lmax)\n\u001b[0m\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_m\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/nimishawalgaonkar/anaconda2/lib/python2.7/site-packages/gpflow-0.4.0-py2.7.egg/gpflow/model.pyc\u001b[0m in \u001b[0;36msample\u001b[0;34m(self, num_samples, Lmin, Lmax, epsilon, thin, burn, verbose, return_logprobs, RNG)\u001b[0m\n\u001b[1;32m    217\u001b[0m                               \u001b[0mLmin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mLmin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLmax\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mLmax\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepsilon\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mthin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mburn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mburn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m                               \u001b[0mx0\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_free_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 219\u001b[0;31m                               return_logprobs=return_logprobs, RNG=RNG)\n\u001b[0m\u001b[1;32m    220\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m     def optimize(self, method='L-BFGS-B', tol=None, callback=None,\n",
      "\u001b[0;32m/Users/nimishawalgaonkar/anaconda2/lib/python2.7/site-packages/gpflow-0.4.0-py2.7.egg/gpflow/hmc.pyc\u001b[0m in \u001b[0;36msample_HMC\u001b[0;34m(f, num_samples, Lmin, Lmax, epsilon, x0, verbose, thin, burn, RNG, return_logprobs)\u001b[0m\n\u001b[1;32m     61\u001b[0m         samples = sample_HMC(f, num_samples=burn, Lmin=Lmin, Lmax=Lmax,\n\u001b[1;32m     62\u001b[0m                              \u001b[0mepsilon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepsilon\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx0\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m                              verbose=verbose, thin=1, burn=0, RNG=RNG)\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"burn-in sampling ended\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/nimishawalgaonkar/anaconda2/lib/python2.7/site-packages/gpflow-0.4.0-py2.7.egg/gpflow/hmc.pyc\u001b[0m in \u001b[0;36msample_HMC\u001b[0;34m(f, num_samples, Lmin, Lmax, epsilon, x0, verbose, thin, burn, RNG, return_logprobs)\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRNG\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLmin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLmax\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mepsilon\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m             \u001b[0mlogprob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m             \u001b[0mlogprob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mlogprob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misnan\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pragma: no cover\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/nimishawalgaonkar/anaconda2/lib/python2.7/site-packages/gpflow-0.4.0-py2.7.egg/gpflow/model.pyc\u001b[0m in \u001b[0;36mobj\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    185\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_feed_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_feed_dict_keys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m             f, g = self.session.run([self._minusF, self._minusG],\n\u001b[0;32m--> 187\u001b[0;31m                                      feed_dict=feed_dict)\n\u001b[0m\u001b[1;32m    188\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/nimishawalgaonkar/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    903\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    904\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 905\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    906\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    907\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/nimishawalgaonkar/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1131\u001b[0m     \u001b[0;31m# TODO(yuanbyu, keveman): Revisit whether we should just treat feeding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m     \u001b[0;31m# of a handle from a different device as an error.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1133\u001b[0;31m     \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_with_movers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1134\u001b[0m     \u001b[0mfinal_fetches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfetch_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1135\u001b[0m     \u001b[0mfinal_targets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfetch_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtargets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/nimishawalgaonkar/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_update_with_movers\u001b[0;34m(self, feed_dict, feed_map)\u001b[0m\n\u001b[1;32m   1388\u001b[0m     \u001b[0mhandle_movers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1389\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mfeed_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfeed_map\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1390\u001b[0;31m       \u001b[0mmover\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msession_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_handle_mover\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1391\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mmover\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1392\u001b[0m         \u001b[0mhandle_movers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeed_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmover\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/nimishawalgaonkar/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/session_ops.pyc\u001b[0m in \u001b[0;36m_get_handle_mover\u001b[0;34m(graph, feeder, handle)\u001b[0m\n\u001b[1;32m    276\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_get_handle_mover\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    277\u001b[0m   \u001b[0;34m\"\"\"Return a move subgraph for this pair of feeder and handle.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 278\u001b[0;31m   \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_handle_feeder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    279\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    280\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/nimishawalgaonkar/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/session_ops.pyc\u001b[0m in \u001b[0;36m_get_handle_feeder\u001b[0;34m(graph, feeder)\u001b[0m\n\u001b[1;32m    255\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_get_handle_feeder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 257\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_feeders\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeeder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/nimishawalgaonkar/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/ops.pyc\u001b[0m in \u001b[0;36mname\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1790\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mc_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_OperationName\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_c_op\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1791\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1792\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_node_def_val\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1793\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1794\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "Aq =  elicit.IntegratedAquisition(X, Y[:,None], X_prime, config_file, model_num, mcmc, reachable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "next_state, next_duel, meanexp, max_exp_imp = Aq.EUI(iter_num, trial_num, savefig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3 is the most important step. It outputs next state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# next_state ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# next duel ?\n",
    "\n",
    "One of the state is always shared between two duels. So, this is nothing but concatenation of next_state with previous."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next_duel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# max EUI ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_exp_imp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4 (Sanity checks)\n",
    "\n",
    "Is our framework on the right track?\n",
    "\n",
    "Check -\n",
    "1. Max Expected Improvement value (is it less than the previous iteration's expected improvement? If so, GOOD!\n",
    "2. Check the Expected Improvement plots. This will be saved automatically in '../data/results/T1/exp_imp_plots/iteration_num'.\n",
    "3. Also check utility samples, how they look? Do they make sense? Is our framework going towards max? Some utility samples will also be saved in '../data/results/T1/utility_samples/iteration_num'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5\n",
    "\n",
    "1. Once you have verified that the framework is on the right track, change the operating temp of the room to the next_state as above. \n",
    "2. Its fine if you are not able to acheive the next state accurately, just record the measured next state value.\n",
    "2. Ask the occupant again, which state does he prefer.\n",
    "3. Add the new measured state and response to the csv  '../data/duels/duels.csv' file.\n",
    "4. Add max_exp_imp to the MEUI column.\n",
    "5. Run the notebook again with updated csv file.\n",
    "6. As you progress with the elicitation, you will notice that the ratio $(MEUI_{(i+1)} - MEUI_{(i)})/MEUI_{(i)}$ will decrease. Based on pilot study, stop the elicitation, once the ratio becomes small enough."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "V = datagen.ThermalPrefDataGen(config_file)\n",
    "Ynew = V.response_gen1D(next_duel[:,None].T)\n",
    "Ynew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Aq.m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Aq.m\n",
    "samples = Aq.samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sample_df = model.get_samples_df(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xnew = np.linspace(20,27,20)[:,None]\n",
    "xx = V.normalize1D(xnew)\n",
    "meanmat = np.zeros(shape = (samples.shape[0], xx.shape[0]))\n",
    "varmat = np.zeros(shape = (samples.shape[0], xx.shape[0]))\n",
    "for i, s in sample_df.iterrows():\n",
    "    model.set_parameter_dict(s)\n",
    "    mean, v = model.predict_f(xx)\n",
    "    var = v[:,:,0]\n",
    "    meanmat[i,:] = mean[:,0]\n",
    "    varmat[i,:] = np.diag(var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def visualize_utility(Xgrid, Mgrid, Vargrid):\n",
    "    \"\"\"\n",
    "    Visualize 1D utility funciton values\n",
    "    Xgrid : grid states\n",
    "    Mgrid : mean of GP at those finite grid points\n",
    "    Vargrid : variance of GP at those finite grid points\n",
    "    \"\"\"\n",
    "    Stdgrid = np.sqrt(Vargrid)\n",
    "    lower = Mgrid - 2*Stdgrid\n",
    "    upper = Mgrid + 2*Stdgrid\n",
    "    #plt.figure(figsize=(12,8))\n",
    "    #plt.plot(Xgrid[:,0], lower, 'g')\n",
    "    #plt.plot(Xgrid[:,0], upper, 'r')\n",
    "    #plt.plot(Xgrid[:,0], Mgrid, 'b')\n",
    "    \n",
    "    line, = plt.plot(Xgrid, Mgrid, lw = 2, color = 'b', label = 'utility', alpha = 0.5)\n",
    "    plt.fill_between(Xgrid[:,0], lower, upper,\n",
    "                     color = line.get_color(), alpha = 0.25)\n",
    "    plt.xlabel('Temperature degC')\n",
    "    plt.ylabel('Utility')\n",
    "    plt.title('Utility at different temp values')\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mini = 10\n",
    "maxi = 60\n",
    "plt.figure(figsize=(12,8))\n",
    "for i in xrange(mini,maxi):\n",
    "    visualize_utility(xnew, meanmat[i,:], varmat[i,:])\n",
    "#plt.legend(loc = 'best')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.stats import norm\n",
    "def diff_norm_g(m, samples, Xgridnorm, Xgrid, mini, maxi):\n",
    "    \"\"\"\n",
    "    Different utilities along with the associated uncertainities\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(12,8))\n",
    "    for i in xrange(mini,maxi):\n",
    "        m.set_state(samples[i,:])\n",
    "        g = m.predict_g_samples(Xgridnorm, 1)\n",
    "        plt.plot(Xgrid, norm.cdf(g[0,:,:]), 'b', lw=2, alpha = 0.25)\n",
    "    a = np.linspace(19.8, 27.2, 100)\n",
    "    plt.plot(a, 0.5*np.ones(a.shape[0]), 'k')\n",
    "    plt.xlim(19.8,27.2)\n",
    "    plt.xlabel('Temperature degC')\n",
    "    plt.ylabel('Indicator')\n",
    "    plt.title('Indicator at different temp values')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_norm_g(model, samples, xx, xnew, 0, 5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.stats import norm\n",
    "def diff_g(m, samples, Xgridnorm, Xgrid, mini, maxi):\n",
    "    \"\"\"\n",
    "    Different utilities along with the associated uncertainities\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(12,8))\n",
    "    for i in xrange(mini,maxi):\n",
    "        m.set_state(samples[i,:])\n",
    "        g = m.predict_g_samples(Xgridnorm, 1)\n",
    "        plt.plot(Xgrid, g[0,:,:], 'b', lw=2, alpha = 0.25)\n",
    "    a = np.linspace(19.8, 27.2, 100)\n",
    "    plt.plot(a, np.zeros(a.shape[0]), 'k')\n",
    "    plt.xlabel('Temperature degC')\n",
    "    plt.ylabel('Latent g')\n",
    "    plt.title('Latent g at different temp values')\n",
    "    plt.xlim(19.8,27.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_g(model, samples, xx, xnew, 0, 2600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.set_state(samples[2])\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_df = model.get_samples_df(samples)\n",
    "sample_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,8))\n",
    "plt.plot(sample_df['unimodal_model.kern_f.lengthscale'])\n",
    "plt.plot(sample_df['unimodal_model.kern_f.signal_variance'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,8))\n",
    "plt.plot(sample_df['unimodal_model.kern_f.lengthscale'],\n",
    "            sample_df['unimodal_model.kern_f.signal_variance'], 'k.', alpha = 0.15)\n",
    "plt.xlabel('signal_lengthscale')\n",
    "plt.ylabel('signal_variance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Aq.X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.X_concat.value.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = model.X.value\n",
    "b = model.X_prime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a[a.shape[0]/2:,:].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = np.array([0.4, -3, 2, 6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a[a > 0] = 1\n",
    "a[a < 0] = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind*-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
